{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klasifikasi Hewan.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtsCQ6OXwwQGuNtQCtJxco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imadegunawinangun/10-Animal-Classification/blob/main/Klasifikasi_Hewan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVXN9_TmOoVa",
        "outputId": "0be89888-9408-4b15-f3c5-d4fcdbc7feb0"
      },
      "source": [
        "!git clone https://github.com/RumahGugun/10-Animal-Classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '10-Animal-Classification'...\n",
            "remote: Enumerating objects: 26199, done.\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBnlyYjNJKha"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-Ot5CNKOvv8"
      },
      "source": [
        "ls 10-Animal-Classification/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEd32tuuRGpu"
      },
      "source": [
        "ls 10-Animal-Classification/raw-img/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck0SAkjxhLSl"
      },
      "source": [
        "base_dir = '10-Animal-Classification/raw-img'\n",
        "\n",
        "cane = os.path.join(base_dir, 'cane')\n",
        "elefante = os.path.join(base_dir, 'elefante')\n",
        "gallina = os.path.join(base_dir, 'gallina')\n",
        "mucca = os.path.join(base_dir, 'mucca')\n",
        "ragno = os.path.join(base_dir, 'ragno')\n",
        "cavallo = os.path.join(base_dir, 'cavallo')\n",
        "farfalla = os.path.join(base_dir, 'farfalla')\n",
        "gatto = os.path.join(base_dir, 'gatto')\n",
        "pecora = os.path.join(base_dir, 'pecora')\n",
        "scoiattolo = os.path.join(base_dir, 'scoiattolo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KBcgAPxRG6W"
      },
      "source": [
        "import ntpath\n",
        "\n",
        "def path_leaf(path):\n",
        "  head, tail = ntpath.split(path)\n",
        "  return tail"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOP6arwiZvGA"
      },
      "source": [
        "def load_image(datadir):\n",
        "  image_path = []\n",
        "  label = []\n",
        "  path_temp = path_leaf(datadir)\n",
        "\n",
        "  for i in range(len(os.listdir(datadir))):\n",
        "    image_path.append(os.path.join(datadir, os.listdir(datadir)[i].strip()))\n",
        "    label.append(path_temp)\n",
        "\n",
        "  image_paths = np.asarray(image_path)\n",
        "  labels = np.asarray(label)\n",
        "  return image_paths, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYRksgYqZ0SO"
      },
      "source": [
        "cane_data, cane_label = load_image(cane)\n",
        "elefante_data, elefante_labels = load_image(elefante)\n",
        "gallina_data, gallina_labels = load_image(gallina)\n",
        "mucca_data, mucca_labels = load_image(mucca)\n",
        "ragno_data, ragno_labels = load_image(ragno)\n",
        "cavallo_data, cavallo_labels = load_image(cavallo)\n",
        "farfalla_data, farfalla_labels = load_image(farfalla)\n",
        "gatto_data, gatto_labels = load_image(gatto)\n",
        "pecora_data, pecora_labels = load_image(pecora)\n",
        "scoiattolo_data, scoiattolo_labels = load_image(scoiattolo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrW2omEWPpJ_"
      },
      "source": [
        "cane_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHf9tkEuPsl7"
      },
      "source": [
        "cane_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcdow7XpTZVU"
      },
      "source": [
        "#JALANKAN UNTUK MENGHAPUS FOLDER TRAINING DAN VALIDATION\n",
        "shutil.rmtree(base_dir+'training')\n",
        "shutil.rmtree(base_dir+'validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1vr_A_ndsm"
      },
      "source": [
        "if not os.path.exists(base_dir+'training'): os.makedirs(base_dir+'training')\n",
        "if not os.path.exists(base_dir+'training/cane'): os.makedirs(base_dir+'training/cane')\n",
        "if not os.path.exists(base_dir+'training/elefante'): os.makedirs(base_dir+'training/elefante')\n",
        "if not os.path.exists(base_dir+'training/gallina'): os.makedirs(base_dir+'training/gallina')\n",
        "if not os.path.exists(base_dir+'training/mucca'): os.makedirs(base_dir+'training/mucca')\n",
        "if not os.path.exists(base_dir+'training/ragno'): os.makedirs(base_dir+'training/ragno')\n",
        "if not os.path.exists(base_dir+'training/cavallo'): os.makedirs(base_dir+'training/cavallo')\n",
        "if not os.path.exists(base_dir+'training/farfalla'): os.makedirs(base_dir+'training/farfalla')\n",
        "if not os.path.exists(base_dir+'training/gatto'): os.makedirs(base_dir+'training/gatto')\n",
        "if not os.path.exists(base_dir+'training/pecora'): os.makedirs(base_dir+'training/pecora')\n",
        "if not os.path.exists(base_dir+'training/scoiattolo'): os.makedirs(base_dir+'training/scoiattolo')\n",
        "\n",
        "if not os.path.exists(base_dir+'validation'):  os.makedirs(base_dir+'validation')\n",
        "if not os.path.exists(base_dir+'validation/cane'): os.makedirs(base_dir+'validation/cane')\n",
        "if not os.path.exists(base_dir+'validation/elefante'): os.makedirs(base_dir+'validation/elefante')\n",
        "if not os.path.exists(base_dir+'validation/gallina'): os.makedirs(base_dir+'validation/gallina')\n",
        "if not os.path.exists(base_dir+'validation/mucca'): os.makedirs(base_dir+'validation/mucca')\n",
        "if not os.path.exists(base_dir+'validation/ragno'): os.makedirs(base_dir+'validation/ragno')\n",
        "if not os.path.exists(base_dir+'validation/cavallo'): os.makedirs(base_dir+'validation/cavallo')\n",
        "if not os.path.exists(base_dir+'validation/farfalla'): os.makedirs(base_dir+'validation/farfalla')\n",
        "if not os.path.exists(base_dir+'validation/gatto'): os.makedirs(base_dir+'validation/gatto')\n",
        "if not os.path.exists(base_dir+'validation/pecora'): os.makedirs(base_dir+'validation/pecora')\n",
        "if not os.path.exists(base_dir+'validation/scoiattolo'): os.makedirs(base_dir+'validation/scoiattolo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA9qNPoiN1di"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "cane_train, cane_valid = train_test_split(cane_data, train_size = 0.8, random_state = 1)\n",
        "print('cane Training Samples: {}\\nValid Samples: {}'.format(len(cane_train), len(cane_valid)))\n",
        "for i in range(len(cane_train)):\n",
        "  if not os.path.exists(base_dir+'training/cane/'+path_leaf(cane_train[i])):\n",
        "    shutil.copyfile(cane_train[i], base_dir+'training/cane/'+path_leaf(cane_train[i]))\n",
        "for i in range(len(cane_valid)):\n",
        "  if not os.path.exists(base_dir+'validation/cane/'+path_leaf(cane_valid[i])):\n",
        "    shutil.copyfile(cane_valid[i], base_dir+'validation/cane/'+path_leaf(cane_valid[i]))\n",
        "\n",
        "elefante_train, elefante_valid = train_test_split(elefante_data, train_size = 0.8, random_state = 1)\n",
        "print('elefante Training Samples: {}\\nValid Samples: {}'.format(len(elefante_train), len(elefante_valid)))\n",
        "for i in range(len(elefante_train)):\n",
        "  if not os.path.exists(base_dir+'training/elefante/'+path_leaf(elefante_train[i])):\n",
        "    shutil.copyfile(elefante_train[i], base_dir+'training/elefante/'+path_leaf(elefante_train[i]))\n",
        "for i in range(len(elefante_valid)):\n",
        "  if not os.path.exists(base_dir+'validation/elefante/'+path_leaf(elefante_valid[i])):\n",
        "    shutil.copyfile(elefante_valid[i], base_dir+'validation/elefante/'+path_leaf(elefante_valid[i]))\n",
        "\n",
        "gallina_train, gallina_valid = train_test_split(gallina_data, train_size = 0.8, random_state = 1)\n",
        "print('gallina Training Samples: {}\\nValid Samples: {}'.format(len(gallina_train), len(gallina_valid)))\n",
        "for i in range(len(gallina_train)):\n",
        "  shutil.copyfile(gallina_train[i], base_dir+'training/gallina/'+path_leaf(gallina_train[i]))\n",
        "for i in range(len(gallina_valid)):\n",
        "  shutil.copyfile(gallina_valid[i], base_dir+'validation/gallina/'+path_leaf(gallina_valid[i]))\n",
        "\n",
        "mucca_train, mucca_valid = train_test_split(mucca_data, train_size = 0.8, random_state = 1)\n",
        "print('mucca Training Samples: {}\\nValid Samples: {}'.format(len(mucca_train), len(mucca_valid)))\n",
        "for i in range(len(mucca_train)):\n",
        "  shutil.copyfile(mucca_train[i], base_dir+'training/mucca/'+path_leaf(mucca_train[i]))\n",
        "for i in range(len(mucca_valid)):\n",
        "  shutil.copyfile(mucca_valid[i], base_dir+'validation/mucca/'+path_leaf(mucca_valid[i]))\n",
        "\n",
        "ragno_train, ragno_valid = train_test_split(ragno_data, train_size = 0.8, random_state = 1)\n",
        "print('ragno Training Samples: {}\\nValid Samples: {}'.format(len(ragno_train), len(ragno_valid)))\n",
        "for i in range(len(ragno_train)):\n",
        "  shutil.copyfile(ragno_train[i], base_dir+'training/ragno/'+path_leaf(ragno_train[i]))\n",
        "for i in range(len(ragno_valid)):\n",
        "  shutil.copyfile(ragno_valid[i], base_dir+'validation/ragno/'+path_leaf(ragno_valid[i]))\n",
        "\n",
        "cavallo_train, cavallo_valid = train_test_split(cavallo_data, train_size = 0.8, random_state = 1)\n",
        "print('cavallo Training Samples: {}\\nValid Samples: {}'.format(len(cavallo_train), len(cavallo_valid)))\n",
        "for i in range(len(cavallo_train)):\n",
        "  shutil.copyfile(cavallo_train[i], base_dir+'training/cavallo/'+path_leaf(cavallo_train[i]))\n",
        "for i in range(len(cavallo_valid)):\n",
        "  shutil.copyfile(cavallo_valid[i], base_dir+'validation/cavallo/'+path_leaf(cavallo_valid[i]))\n",
        "\n",
        "farfalla_train, farfalla_valid = train_test_split(farfalla_data, train_size = 0.8, random_state = 1)\n",
        "print('farfalla Training Samples: {}\\nValid Samples: {}'.format(len(farfalla_train), len(farfalla_valid)))\n",
        "for i in range(len(farfalla_train)):\n",
        "  shutil.copyfile(farfalla_train[i], base_dir+'training/farfalla/'+path_leaf(farfalla_train[i]))\n",
        "for i in range(len(farfalla_valid)):\n",
        "  shutil.copyfile(farfalla_valid[i], base_dir+'validation/farfalla/'+path_leaf(farfalla_valid[i]))\n",
        "  \n",
        "gatto_train, gatto_valid = train_test_split(gatto_data, train_size = 0.8, random_state = 1)\n",
        "print('gatto Training Samples: {}\\nValid Samples: {}'.format(len(gatto_train), len(gatto_valid)))\n",
        "for i in range(len(gatto_train)):\n",
        "  shutil.copyfile(gatto_train[i], base_dir+'training/gatto/'+path_leaf(gatto_train[i]))\n",
        "for i in range(len(gatto_valid)):\n",
        "  shutil.copyfile(gatto_valid[i], base_dir+'validation/gatto/'+path_leaf(gatto_valid[i]))\n",
        "  \n",
        "pecora_train, pecora_valid = train_test_split(pecora_data, train_size = 0.8, random_state = 1)\n",
        "print('pecora Training Samples: {}\\nValid Samples: {}'.format(len(pecora_train), len(pecora_valid)))\n",
        "for i in range(len(pecora_train)):\n",
        "  shutil.copyfile(pecora_train[i], base_dir+'training/pecora/'+path_leaf(pecora_train[i]))\n",
        "for i in range(len(pecora_valid)):\n",
        "  shutil.copyfile(pecora_valid[i], base_dir+'validation/pecora/'+path_leaf(pecora_valid[i]))\n",
        "  \n",
        "scoiattolo_train, scoiattolo_valid = train_test_split(scoiattolo_data, train_size = 0.8, random_state = 1)\n",
        "print('scoiattolo Training Samples: {}\\nValid Samples: {}'.format(len(scoiattolo_train), len(scoiattolo_valid)))\n",
        "for i in range(len(scoiattolo_train)):\n",
        "  shutil.copyfile(scoiattolo_train[i], base_dir+'training/scoiattolo/'+path_leaf(scoiattolo_train[i]))\n",
        "for i in range(len(scoiattolo_valid)):\n",
        "  shutil.copyfile(scoiattolo_valid[i], base_dir+'validation/scoiattolo/'+path_leaf(scoiattolo_valid[i]))\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42REKaouaDjO"
      },
      "source": [
        "train_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rotation_range=90,\n",
        "    height_shift_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    brightness_range = (0.5,1.2),\n",
        ")\n",
        "\n",
        "valid_generator = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIO0hMKbaHxi"
      },
      "source": [
        "IMAGE_WIDTH=128\n",
        "IMAGE_HEIGHT=128\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS=3\n",
        "IMAGE_SHAPE = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 20\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "train_images = train_generator.flow_from_directory(\n",
        "    base_dir+'training',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=10\n",
        ")\n",
        "\n",
        "val_images = valid_generator.flow_from_directory(\n",
        "    base_dir+'validation',\n",
        "    target_size=IMAGE_SIZE,\n",
        "    class_mode='categorical',\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTHAVwcIjOB0"
      },
      "source": [
        "for _ in range(5):\n",
        "    img, label = train_images.next()\n",
        "    print(img.shape)   #  (1,256,256,3)\n",
        "    plt.imshow(img[1])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzsF_kn6hpRW"
      },
      "source": [
        "plt.figure(figsize=(40, 40))\n",
        "\n",
        "for i in range(160):\n",
        "    plt.subplot(40, 40, i + 1)\n",
        "    img = train_images.next()[0][0]\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPthugU3HRx6"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMY7XVRJcJI6"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class ModifiedMobileNet():\n",
        "    '''\n",
        "    This class creates the mobilenet model.\n",
        "    '''\n",
        "    # konfigurasi object\n",
        "    def __init__(self, input_shape, nb_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.nb_classes = nb_classes\n",
        "    \n",
        "    # fungsi untuk membangun model dengan MobilenetV2\n",
        "    def get_model(self, unfreeze_layers = None, lr_rate = 0.001):\n",
        "        \n",
        "        # Memuat model MobileNetV2 dengan tidak meninclude top layer, dan input shape yang disesuaikan oleh parameter input_shape\n",
        "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
        "        \n",
        "        # freezing layers\n",
        "        for layer in (base_model.layers) if not unfreeze_layers else (base_model.layers[:-int(unfreeze_layers)]):\n",
        "            layer.trainable = False\n",
        "        \n",
        "        #inputan untuk model berupa shape\n",
        "        inputs = Input(shape=self.input_shape)\n",
        "\n",
        "        #memasukan input layer\n",
        "        x = base_model(inputs, training=False)\n",
        "        #menambah layer MaxPooling2D ke dalam base_model\n",
        "        x = tf.keras.layers.MaxPooling2D()(x)\n",
        "        #menambah layer Dense ke dalam base_model\n",
        "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "        #menambah layer Dropout ke dalam base_model\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        #menambah layer Dense ke dalam base_model\n",
        "        x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "        #menambah layer Dropout ke dalam base_model\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        #layer output Dense dengan filter sebanyak label class\n",
        "        outputs = tf.keras.layers.Dense(self.nb_classes, activation='softmax')(x)\n",
        "        #menggabungkan inputs dan outputs kedalam Model\n",
        "        model = Model(inputs, outputs)\n",
        "        \n",
        "        # model compilation\n",
        "        optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yH6rDWkuCfG"
      },
      "source": [
        "\n",
        "\n",
        "mobilenet = ModifiedMobileNet(input_shape= IMAGE_SHAPE, nb_classes= 10)\n",
        "model2 = mobilenet.get_model(unfreeze_layers=0, lr_rate= LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJqu4OVVm-Q3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Bupd4WuyU2"
      },
      "source": [
        "history = model2.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    epochs=100,\n",
        "    validation_steps=val_images.n//val_images.batch_size,\n",
        "    steps_per_epoch=train_images.n // train_images.batch_size,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_accuracy',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KFgDJd0wACJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}